\section{Training Feedforward Neural Networks}
\label{sec:training}
The training process of neural networks utilizes the same ideas that we saw in the perceptron model. In particular, we are again given a set of $m$ training examples $\mathbb{X}$ with corresponding labels $\mathbb{Y}$, and wish to iteratively adjust the parameters $\bm{\theta}$ of the neural network to learn a mapping from $\mathbb{X}$ to $\mathbb{Y}$.

For example, in a multiclass classification setting where we want to predict which digit is shown on an input image, the training examples $\bm{x}^{(i)}$ might be a flattened vector of the raw pixel values and the labels $\bm{y}^{(i)}$ might encode the corresponding class in a \emph{one-hot} fashion. This means that the number of dimensions of $\bm{y}^{(i)}$ matches the number of classes $k$, and if the \nth{i} entry of $\bm{y}^{(i)}$ equals 1, the vector represents the \nth{i} class. All other entries are equal to 0.
%TODO: keep this??

Suppose we are given such a set of training data and a neural network $f(\bm{\theta})$ with parameters $\bm{\theta}$, such that the output $\hat{\bm{y}} = f(\bm{x}; \bm{\theta})$ for an input vector $\bm{x}$. In this section, we explore how we can choose $\bm{\theta}$ in order to learn the function described by the training data.
%TODO: cite Goodfellow here?

\subsection{Cost functions}
Since we start with a random set of parameters and wish to improve them, we need some kind of measure of how good the network performs. For this, we introduce the \emph{cost function} $J(\bm{\theta})$, sometimes also referred to as \emph{loss} or \emph{error} function. $J(\bm{\theta})$ is non-negative and the closer it is to 0, the better our network performs. We can thus reframe the training problem as \emph{minimizing the cost function}.

The cost functions that we will consider can all be represented as sums over the costs of the individual training examples. Thus, we can write $J(\bm{\theta})$ as
\begin{equation}
J(\bm{\theta}) = \frac1{m}\sum_{i=1}^{m}L(\bm{x}^{(i)},\bm{y}^{(i)},\bm{\theta}),
\end{equation}
where $L$ is the loss for an individual training example \cite[Ch.\,5,\,pp.\,147-148]{DBLP:books/daglib/0040158}. We scale $J$ by $1/m$ to make the cost independent of the number of training examples $m$.

We choose the particular per-example cost function $L$ based on the task that we wish to perform.

In regression, a good choice is the \emph{mean squared error}
\begin{equation}
L(\bm{x}, y, \bm{\theta}) = \frac1{2}\lVert\hat{y}-y\rVert^2,
\end{equation}
which simply corresponds to the distance of the desired output from the actual output. It can easily be seen that it satisfies the properties of a cost function.

The mean squared error is also a valid cost function in classification problems. However, it has some undesirable properties that can make training very difficult in this setting \cite[Ch.\,6,\,p.\,178]{DBLP:books/daglib/0040158}. Thus, we choose a different cost function, called the \emph{cross-entropy}, which is defined as
\begin{equation}
L(\bm{x}, y, \bm{\theta}) = -y \ln \hat{y} - (1-y)\ln(1-\hat{y})
\end{equation}
for binary classification tasks, where we specify the labels $y$ to be either 0 or 1. We can see that, if $y=1$, the loss simply becomes $-\ln\hat{y}$, which is large if $\hat{y}$ is close to 0, and small if $\hat{y}$ is close to 1. A similar analysis can be conducted for the case $y=0$ and we can see that this cost function also satisfies to property that it is close to 0 if our prediction is almost correct.

In case of multiclass classification, the cross-entropy becomes
\begin{equation}
L(\bm{x}, \bm{y}, \bm{\theta}) = -\ln \hat{y}_i
\end{equation}
if $\bm{y}$ represents the \nth{i} class (i.e. the \nth{i} position in the vector $\bm{y}$ is 1). Similar arguments as in the case of binary classification apply as to why this is a valid cost function.

While these functions might not look very similar, they can all be derived with the same principle, called \emph{maximum likelihood estimation} (MLE) \cite[Ch.\,5,\,pp.\,128-131]{DBLP:books/daglib/0040158}. This estimation comes from a probabilistic perspective and interprets the training data as samples drawn from an unknown probability distribution $P(\bm{Y}\given\bm{X})$. From this perspective, the goal of training is to choose a set of parameters $\bm{\theta}$ such that the probability distribution $P_{\text{model}}(\bm{Y}\given\bm{X}; \bm{\theta})$ described by the model matches the true distribution as closely as possible. The MLE states that we should choose the parameters that maximize $P_{\text{model}}$. The optimal set of parameters $\hat{\bm{\theta}}$ is thus given by
\begin{equation}
\hat{\bm{\theta}} = \argmax_{\bm{\theta}} P_{\text{model}}(\bm{Y}\given\bm{X}; \bm{\theta}).
\end{equation}

We obtain the mean squared error from the MLE if we regard the true probability distribution of the data as a Gaussian distribution. In the case of binary and multiclass classification, we see the true probability distribution as a Bernoulli or Multinoulli distribution, respectively \cite[ Ch.\,6,\,pp.\,175-185]{DBLP:books/daglib/0040158}.
%TODO: keep all this? or only mention MLE and move on?

\subsection{Stochastic Gradient Descent}
Minimizing cost functions is similar to any other minimization problem. A variety of algorithms exist to minimize functions, but stochastic gradient descent, an extension of gradient descent, is particularly dominant in neural network training.

As the name suggests, gradient descent makes use of the gradient of the cost function, in order to iteratively find parameters that decrease the cost. In particular, we know from Calculus that a small change $\bm{v}$ in $\bm{\theta}$ corresponds roughly to the change
\begin{equation}
\Delta J(\bm{\theta}) \approx \nabla J(\bm{\theta})^{\top}\bm{v}
\end{equation}
in $J(\bm{\theta})$. It can be shown that the choice of $\bm{v}$ which decreases the error the fastest and thus minimizes $\Delta J(\bm{\theta})$ is
\begin{equation}
\bm{v} = -\eta\nabla J(\bm{\theta}),
\end{equation}
where $\eta$ is the \emph{learning rate}. Hence, gradient descent iteratively chooses the point
\begin{equation}
\bar{\bm{\theta}} = \bm{\theta} - \eta\nabla J(\bm{\theta}).
\end{equation}

Impact of learning rate.

Stochastic gradient descent.

Variations.

\begin{comment}
Applying the Cauchy-Schwarz inequality, we obtain
\begin{equation}\label{eq:cauchy-schwarz}
-\lVert\nabla J(\bm{\theta})\rVert\lVert v\rVert \leq
\nabla J(\bm{\theta})^{\top}\bm{v} \leq
\lVert\nabla J(\bm{\theta})\rVert\lVert v\rVert.
\end{equation}
We observe that, if we set $\bm{v} = -\eta\nabla J(\bm{\theta})$, then
\begin{equation}
\nabla J(\bm{\theta})^{\top}\bm{v} = -\eta\lVert\nabla J(\bm{\theta})\rVert^2,
\end{equation}
which is exactly the left-hand side of Eq. \eqref{eq:cauchy-schwarz}. Thus, in order to minimize $\Delta J(\bm{\theta})$, we can choose new parameters $\bar{\bm{\theta}}$ with the update rule
\begin{equation}
\bar{\bm{\theta}} = \bm{\theta} - \eta\nabla J(\bm{\theta}),
\end{equation}
\end{comment}

\subsection{The Back-propagation Algorithm}
Backprop.

Complete training algorithm.