\section{Training Feedforward Neural Networks}
\label{sec:training}
Similar to the perceptron model, when training neural networks, we have a set of $m$ training examples $\mathbb{X} = \{\bm{x}^{(1)}, \ldots, \bm{x}^{(m)}\}$ with corresponding labels $\mathbb{Y}$, and wish to iteratively adjust the parameters $\bm{\theta}$ of the neural network to learn a mapping from $\mathbb{X}$ to $\mathbb{Y}$. The parameters are usually randomly initialized.

In a binary classification setting, the labels $y^{(i)}$ are either 0 or 1 to indicate one of the two classes. In multiclass problems, they encode one of $k$ classes in a \emph{one-hot} fashion: in this case, $\bm{y}^{(i)}$ is a $k$-dimensional vector whose \nth{i} entry is 1 if $\bm{y}^{(i)}$ represents the \nth{i} class. All other entries are equal to 0. Finally, in regression, the labels are simply the scalar values that we want to predict.

Suppose we are given such a set of training data and a neural network $f(\bm{\theta})$. In this section, we explore how we can choose $\bm{\theta}$ in order that the network learns the mapping described by the training data, mainly utilizing concepts presented in Ref. \cite{DBLP:books/daglib/0040158} and \cite{Nielsen2015}.

\subsection{Cost functions}
Since we start with a random set of parameters which we wish to improve, we need some kind of measure of how good the network performs. For this, we introduce the \emph{cost function} $J(\bm{\theta})$, sometimes also referred to as \emph{loss} or \emph{error} function. $J(\bm{\theta})$ produces a scalar cost which is non-negative and the closer it is to 0, the better our network performs. We can thus reframe the training problem as minimizing the cost function.

The cost functions that we will consider can all be represented as sums over the costs of the individual training examples:
\begin{equation}
J(\bm{\theta}) = \frac1{m}\sum_{i=1}^{m}\loss(\bm{x}^{(i)},\bm{y}^{(i)},\bm{\theta}),
\end{equation}
where $\loss$ is the loss for an individual training example \cite[Ch.\,5,\,pp.\,147-148]{DBLP:books/daglib/0040158}. We scale $J$ by $1/m$ to make the cost independent of the number of training examples $m$.

The particular per-example cost function $\loss$ is chosen based on the task that we wish to perform.

In regression, a good choice is the \emph{mean squared error}
\begin{equation}
\loss(\bm{x}, y, \bm{\theta}) = \frac1{2}(\hat{y}-y)^2,
\end{equation}
which simply corresponds to the distance of the desired output from the actual output. It can easily be seen that it satisfies the properties of a cost function: it is always non-negative and if $\hat{y}$ is similar to $y$, then $(\hat{y}-y)^2/2 \approx 0$.

The mean squared error is also a valid cost function in  binary classification problems. However, it has some undesirable properties that can make training very difficult in this setting \cite[Ch.\,6,\,p.\,178]{DBLP:books/daglib/0040158}. Hence, a different cost function, called the \emph{cross-entropy}, is commonly chosen. The cross-entropy loss is defined as
\begin{equation}
\loss(\bm{x}, y, \bm{\theta}) = -y \ln \hat{y} - (1-y)\ln(1-\hat{y}).
\end{equation}
We can see that, if $y=1$, the loss simply becomes $-\ln\hat{y}$, which is large if $\hat{y}$ is close to 0, and small if $\hat{y}$ is close to 1. A similar analysis can be conducted for the case $y=0$ and we can see that this cost function also satisfies the desired properties.

In case of multiclass classification, the cross-entropy becomes
\begin{equation}
\loss(\bm{x}, \bm{y}, \bm{\theta}) = -\ln \hat{y}_i
\end{equation}
if $\bm{y}$ represents the \nth{i} class (i.e. the \nth{i} position in the vector $\bm{y}$ is 1). Similar arguments as in the case of binary classification apply as to why this is a valid cost function.

While these functions might seem rather different, they can all be derived with the same principle, called \emph{maximum likelihood estimation} (MLE) \cite[Ch.\,5,\,pp.\,128-131]{DBLP:books/daglib/0040158}. This estimation comes from a probabilistic perspective and interprets the training data as samples drawn from an unknown probability distribution $P(\mathbb{Y}\given\mathbb{X})$. From this perspective, the goal of training is to choose parameters $\bm{\theta}$ such that the probability distribution $P_{\text{model}}(\mathbb{Y}\given\mathbb{X}; \bm{\theta})$ described by the model matches the true distribution as closely as possible. The MLE states that we should choose the parameters that maximize $P_{\text{model}}$, i.e. the optimal parameters $\hat{\bm{\theta}}$ are
\begin{equation}
\hat{\bm{\theta}} = \argmax_{\bm{\theta}} P_{\text{model}}(\mathbb{Y}\given\mathbb{X}; \bm{\theta}).
\end{equation}

We obtain the mean squared error from the MLE if we regard the true probability distribution of the data as a Gaussian distribution. In the case of binary and multiclass classification, we regard the true probability distribution as a Bernoulli or Multinoulli distribution, respectively \cite[ Ch.\,6,\,pp.\,175-185]{DBLP:books/daglib/0040158}.
%TODO: keep all this? or only mention MLE and move on?

\subsection{Stochastic Gradient Descent}
Minimizing cost functions is similar to any other minimization problem. A variety of algorithms exist to minimize functions, but stochastic gradient descent, an extension of gradient descent, is particularly dominant in neural network training.

As the name suggests, gradient descent makes use of the gradient of the cost function to iteratively improve the parameters to decrease the cost. In particular, we know from calculus that a small change $\Delta\bm{\theta}$ in $\bm{\theta}$ corresponds roughly to the change
\begin{equation}
\Delta J(\bm{\theta}) \approx \nabla J(\bm{\theta})^{\top}\Delta\bm{\theta}
\end{equation}
in $J(\bm{\theta})$. It can be shown that the choice of $\Delta\bm{\theta}$ which decreases the cost the fastest and thus minimizes $\Delta J(\bm{\theta})$ is
\begin{equation}
\Delta\bm{\theta} = -\eta\nabla J(\bm{\theta}),
\end{equation}
where $\eta$ is the \emph{learning rate} \cite[ Ch.\,4,\,p.\,82]{DBLP:books/daglib/0040158}. This means that we can improve a neural network by making small changes to the parameters in the negative direction of the gradient of the cost function.

The learning rate $\eta$ controls the size of the update steps performed by gradient descent. It should neither be too small, nor too large: if it is small, the model only learns very slowly, and if it is large, we risk making updates that unintentionally increase the loss, since the gradient is only an approximation of the real cost function.

The problem with using gradient descent to train neural networks is that computing the gradient is linear in the number of training examples $m$. To see this, note that
\begin{equation}
\nabla J(\bm{\theta}) = \frac1{m}\sum_{i=1}^{m}\nabla \loss(\bm{x}^{(i)},\bm{y}^{(i)},\bm{\theta}).
\end{equation}
\emph{Stochastic} gradient descent resolves this dependency by computing an approximation of the gradient by averaging over only a subset of training examples $\mathbb{B}$ called a \emph{minibatch}. The gradient computation becomes
\begin{equation}
\nabla J(\bm{\theta}) = \frac1{|\mathbb{B}|}\sum_{(\bm{x},\bm{y})\in\mathbb{B}}\nabla \loss(\bm{x},\bm{y},\bm{\theta}),
\end{equation}
which is independent of $m$. The size $|\mathbb{B}|$ of the minibatches is not increased with the amount of training data.

Many variations of stochastic gradient descent exist. They usually make use of higher order derivatives or approximations thereof and dynamically adjust the learning rate \cite{DBLP:journals/corr/Ruder16}.

\subsection{The Back-propagation Algorithm}

%TODO: Complete training algorithm.