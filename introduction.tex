\section{Introduction}
Artificial intelligence systems have been becoming more and more powerful over the last 10 years. We have seen outstanding advances in a variety of fields including computer vision, natural language processing and fraud detection, which power many end-user technologies such as digital assistants or self-driving cars. Much of the recent progress can be attributed to \textit{deep learning}, a powerful set of techniques that enable computers to understand the world by decomposing complex concepts into a hierarchy of simpler abstractions.

While numerous other approaches to machine learning exist, deep learning has shown to outperform other methods in a wide variety of applications. To name a few examples, deep learning models dominate the task of object recognition in images \cite{DBLP:journals/ijcv/RussakovskyDSKS15}, even surpassing human-level performance \cite{DBLP:conf/iccv/HeZRS15}, have been successfully applied to sentiment analysis \cite{DBLP:conf/sigir/SeverynM15a}, and
have significantly improved speech recognition systems \cite{DBLP:journals/taslp/MohamedDH12}. Deep learning has also been used in problems such as style transfer between images \cite{DBLP:conf/cvpr/GatysEB16}, image description generation \cite{DBLP:journals/pami/KarpathyF17}, and learning to play video games \cite{DBLP:journals/nature/MnihKSRVBGRFOPB15}.

By learning everything required to solve a task purely from raw data, these techniques have alleviated the need for problem-specific expert knowledge. Thus, very similar models building on the same core ideas can be applied to a vast array of different tasks with outstanding success.

One such core idea that is fundamental to deep learning is the \textit{neural network}, a computing model loosely inspired by neuroscience. While neural networks are not new, it was not until recently that enough data and computational resources became available to train them effectively and fully appreciate their power \cite[Ch.\,1,\,pp.\,18-21]{DBLP:books/daglib/0040158}.

Since neural networks have become so prevalent in modern machine learning applications, many libraries exist that abstract their concepts and provide simple programming interfaces. However, it does not suffice to be familiar with such libraries to use neural networks effectively; in order to understand which architectures perform well, and why, one must also know their mathematical foundations.

In this paper we thus aim to give a thorough overview of neural networks and the fundamental techniques and algorithms associated with them. We first briefly examine the motivation and history behind neural networks in Section \ref{sec:perceptron} by introducing the \textit{perceptron} model. Section \ref{sec:feedforward_neural_networks} then shows how this model has been adjusted and extended to obtain the neural network, focusing in particular on \textit{feedforward neural networks}. In Section \ref{sec:training}, we then proceed to explain how these networks can be trained, introducing ideas such as \textit{stochastic gradient descent} and \textit{back-propagation}. Subsequently, Section \ref{sec:approximation} discusses the effectiveness of neural networks from a theoretical point of view. In Section \ref{sec:extensions} we examine several extensions to the basic feedforward neural network that are often used in practice, before we conclude our paper in Section \ref{sec:conclusion}.