\section{Introduction}
Artificial Intelligence systems have been becoming more and more powerful over the last 10 years. We have seen outstanding advances in a variety of fields such as computer vision, natural language processing and speech recognition. Much of the recent progress can be attributed to \textit{deep learning}, a set of techniques that enable computers to understand the world by decomposing complex concepts into a hierarchy of simpler concepts.

While numerous other approaches to machine learning exist, deep learning has shown to outperform other methods in a wide variety of fields. To name a few examples, deep learning models dominate the task of object recognition in images \cite{DBLP:journals/ijcv/RussakovskyDSKS15}, even surpassing human-level performance \cite{DBLP:conf/iccv/HeZRS15}, have been successfully applied to language identification \cite{DBLP:conf/interspeech/Gonzalez-DominguezLSGM14} and
have significantly improved speech recognition systems \cite{DBLP:journals/taslp/MohamedDH12}. Deep learning has also been applied to problems such as style transfer between images \cite{DBLP:conf/cvpr/GatysEB16}, image description generation \cite{DBLP:journals/pami/KarpathyF17}, and learning to play video games \cite{DBLP:journals/nature/MnihKSRVBGRFOPB15}.

\hl{While} other machine learning models often require domain-specific expert knowledge and carefully crafted features, an inherent property of the hierarchical architecture of deep learning models is that they are able to build representations of complex features in terms of simpler features. This ability to learn feature representations themselves instead of only operating on the given features is one of the reasons why deep learning models are so powerful and widely used in different applications.

This also implies that the models themselves are not engineered to a specific task--indeed almost all modern deep learning models are based on the same underlying architecture called a \textit{neural network}.

\todo[inline]{Important to understand this underlying principle. In this paper overview: Historical perceptron, then ..., .}