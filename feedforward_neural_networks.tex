\section{Feedforward Neural Networks}
\label{sec:feedforward_neural_networks}
A natural extension of the perceptron model is to combine multiple perceptrons in a network architecture. It is intuitively clear that, much like in an organic brain, a complex arrangement of many simple computing units can learn much more complicated functions than those simple units alone. In this section, we will examine how such a network architecture based on perceptrons can be constructed. The ideas that we develop are mostly based on Ref. \cite[Ch. 6]{DBLP:books/daglib/0040158}.

\subsection{Extensions to the Perceptron}
Before explaining the composition of perceptrons to neural networks, we will first explore two extensions to the perceptron model that are common in neural networks.

First, we introduce an additional term called \emph{bias} to the output calculation. The new output $\hat{y}$ becomes
\begin{equation}
\hat{y} = \sign(\bm{w}^\top\bm{x} + b),
\end{equation}
where the scalar $b$ is the bias. This additional learnable parameter shifts the function computed by the model independently of its input. If the bias is large and positive, the model is more inclined to predict a positive label, while a negative bias makes it more likely that negative labels are predicted.

Second, we generalize the perceptron by replacing the $\sign$ function with an arbitrary function $f$ called \emph{activation function}. In contrast to $\sign(x)$, most activation functions used in neural networks are continuous, since this enables us to use a variety of \emph{gradient-based} learning algorithms for training as we will see in Section \ref{sec:training}. Concrete examples of activation functions will be discussed later in this section.

The computing units we have obtained with these modifications to the perceptron are generally called \emph{neurons} or simply \emph{units}.

\begin{comment}
One common activation function is the \emph{logistic sigmoid}:

\begin{equation}
\sigma(x) = \frac1{1+e^{-x}}.
\end{equation}

\begin{figure}
	\begin{center}
		\input{sigmoid_fig}
	\end{center}
	\caption{The logistic sigmoid function.}
	\label{fig:sigmoid}
\end{figure}

As shown in Fig. \ref{fig:sigmoid}, $\sigma(x)$ squashes the output to a value between 0 and 1. A very similar function, producing an output between -1 and 1, is $\tanh(x)$. Lastly, one of the most commonly used activation functions is the \emph{rectified linear} function $g(x) = \max\{0,x\}$ plotted in Fig. \ref{fig:relu}.

\begin{figure}
	\begin{center}
		\input{relu_fig}
	\end{center}
	\caption{The rectified linear function.}
	\label{fig:relu}
\end{figure}

The choice of activation function is an important design decision that can greatly influence how well the model learns. The rectified linear function is a very good default choice, as it has shown to outperform other functions in a variety of different model architectures \cite{DBLP:journals/jmlr/GlorotBB11 ,DBLP:conf/nips/KrizhevskySH12}. We will see why some functions work better than others in Section \ref{sec:training}.
\end{comment}

\subsection{Network architecture}
Any arrangement of neurons in a network architecture can be considered a neural network. The most influential such architecture is the feedforward neural network, which forms the basis for many other more advanced neural networks \cite[Ch.6, p. 163]{DBLP:books/daglib/0040158}. Feedforward neural networks are sometimes also called \emph{multilayer perceptrons} (MLPs).

In feedforward neural networks, the computing units are arranged in layers. We distinguish between the \emph{output layer}, the \emph{hidden layers}, and the \emph{input layer}. The output layer is the final layer in the network were its actual output is produced. The input layer is the first layer in the network, and all layers in between are called hidden layers. The input layer is special as it does not compute anything; it merely represents the input that is passed into the neural network. In general, a $L$-layer feedforward neural network consists of one input layer, $L-2$ hidden layers, and an output layer. We call the number of layers $L$ the \emph{depth} of the model.

Every neuron in a layer $l$ receives input from all neurons in the \nth{(l-1)} layer. There are no connections between neurons in the same layer, and we also do not allow feedback connections into previous layers. Neurons in the hidden and output layers behave exactly like the modified perceptron, the only important detail is that their input is the output from the previous layer.

An illustration of a feedforward neural network can be found in Fig. \ref{fig:network}.

\begin{figure}
	\begin{center}
		\input{network_fig}
	\end{center}
	\caption{A three-layer neural network. The network accepts an input $\bm{x} \in \mathbb{R}^4$, propagates it through its hidden layer, and finally produces an output $\hat{\bm{y}} \in \mathbb{R}^2$.}
	\label{fig:network}
\end{figure}